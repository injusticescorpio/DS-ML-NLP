{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAG OF WORDS ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\"Hello.\n",
    "Iam a medical chatbot named Arista.\n",
    "Iam Rasa Powered AI chatbot capable of telling the symptoms, precaution of various diseases.\n",
    "Iam also capable of booking a particular hospital.\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data preprocessing</h1>\n",
    "\n",
    "<ul>\n",
    "    <li><h2>Convert text to lowercase</h2></li>\n",
    "    <li><h2>Remove all non-word characters</h2></li>\n",
    "    <li><h2>Remove punctuations</h2></li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Hello.',\n",
       " 'Iam a medical chatbot named Arista.',\n",
       " 'Iam Rasa Powered AI chatbot capable of telling the symptoms, precaution of various diseases.',\n",
       " 'Iam also capable of booking a particular hospital.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    dataset[i]=dataset[i].lower()\n",
    "    dataset[i]=re.sub(r'\\W',' ',dataset[i])\n",
    "    dataset[i]=re.sub(r'\\s',' ',dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' hello ',\n",
       " 'iam a medical chatbot named arista ',\n",
       " 'iam rasa powered ai chatbot capable of telling the symptoms  precaution of various diseases ',\n",
       " 'iam also capable of booking a particular hospital ']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count={}\n",
    "for data in dataset:\n",
    "    sentence=nltk.word_tokenize(data)\n",
    "    for word in sentence:\n",
    "        if word not in word_count.keys():\n",
    "            word_count[word]=1\n",
    "        else:\n",
    "            word_count[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 1,\n",
       " 'iam': 3,\n",
       " 'a': 2,\n",
       " 'medical': 1,\n",
       " 'chatbot': 2,\n",
       " 'named': 1,\n",
       " 'arista': 1,\n",
       " 'rasa': 1,\n",
       " 'powered': 1,\n",
       " 'ai': 1,\n",
       " 'capable': 2,\n",
       " 'of': 3,\n",
       " 'telling': 1,\n",
       " 'the': 1,\n",
       " 'symptoms': 1,\n",
       " 'precaution': 1,\n",
       " 'various': 1,\n",
       " 'diseases': 1,\n",
       " 'also': 1,\n",
       " 'booking': 1,\n",
       " 'particular': 1,\n",
       " 'hospital': 1}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For most sequent word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "freq_words=heapq.nlargest(10,word_count,key=word_count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iam',\n",
       " 'of',\n",
       " 'a',\n",
       " 'chatbot',\n",
       " 'capable',\n",
       " 'hello',\n",
       " 'medical',\n",
       " 'named',\n",
       " 'arista',\n",
       " 'rasa']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "for data in dataset:\n",
    "    vector=[]\n",
    "    for word in freq_words:\n",
    "        if word not in nltk.word_tokenize(data):\n",
    "            vector.append(0)\n",
    "        else:\n",
    "            vector.append(1)\n",
    "    x.append(vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "array=np.asarray(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 1, 1, 1, 0],\n",
       "       [1, 1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 1, 1, 0, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
